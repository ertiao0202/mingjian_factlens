# -*- coding: utf-8 -*-
"""
æ˜é‰´ FactLens â€“ äº‹å®è§‚ç‚¹åˆ†æå·¥å…·
v1.0.0  2024-10
"""

import streamlit as st
import requests
import re
import pandas as pd
from bs4 import BeautifulSoup
import time

# -------------------- é¡µé¢é…ç½® --------------------
st.set_page_config(
    page_title="æ˜é‰´ FactLens - äº‹å®è§‚ç‚¹åˆ†æ",
    page_icon="ğŸ”",
    layout="wide"
)

# -------------------- æ ·å¼ --------------------
st.markdown("""
<style>
    .brand-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .lens-fact {
        background-color: #e6f7ff !important;
        border-left: 4px solid #1890ff !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .lens-opinion {
        background-color: #fff7e6 !important;
        border-left: 4px solid #ffa940 !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .lens-mixed {
        background-color: #f6ffed !important;
        border-left: 4px solid #52c41a !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    @media (max-width: 768px) {
        .stColumn { width: 100% !important; }
        .brand-header h1 { font-size: 24px !important; }
        .brand-header h3 { font-size: 18px !important; }
    }
</style>
""", unsafe_allow_html=True)

# -------------------- å“ç‰Œå¤´éƒ¨ --------------------
st.markdown("""
<div class="brand-header">
    <h1>ğŸ” æ˜é‰´ FactLens</h1>
    <h3>æ˜é‰´äº‹å®ï¼Œæ´è§è§‚ç‚¹</h3>
    <p>AIé©±åŠ¨çš„æ–°é—»å†…å®¹æ™ºèƒ½åˆ†æå·¥å…·</p>
</div>
""", unsafe_allow_html=True)

# -------------------- è¾“å…¥ --------------------
st.header("ğŸ“° æ–°é—»åˆ†æ")
url = st.text_input("è¯·è¾“å…¥æ–°é—»ç½‘å€ï¼š", placeholder="https://example.com/news/ ...")

# -------------------- è§„åˆ™å¼•æ“ --------------------
def simple_analyze(sentence: str):
    sentence_lower = sentence.lower()
    fact_kw = ['æ•°æ®', 'ç»Ÿè®¡', 'æŠ¥å‘Š', 'æ˜¾ç¤º', 'è¡¨ç¤º', 'è¯å®', 'æ®', 'ç ”ç©¶',
               'è°ƒæŸ¥', 'ç»“æœ', 'ç™¾åˆ†æ¯”', 'å¢é•¿', 'ä¸‹é™', 'æ—¥æœŸ', 'æ—¶é—´', 'åœ°ç‚¹']
    opinion_kw = ['è®¤ä¸º', 'è§‰å¾—', 'åº”è¯¥', 'å¯èƒ½', 'æˆ–è®¸', 'ç›¸ä¿¡', 'å»ºè®®', 'æˆ‘',
                  'æˆ‘ä»¬', 'å¿…é¡»', 'ä¸€å®š', 'è‚¯å®š', 'æ˜¾ç„¶', 'æ¯«æ— ç–‘é—®', 'ç¾å¥½', 'ç³Ÿç³•']
    fact_score = sum(sentence_lower.count(w) for w in fact_kw)
    opinion_score = sum(sentence_lower.count(w) for w in opinion_kw)

    if fact_score > opinion_score:
        return "äº‹å®", 0.7 + fact_score * 0.1
    elif opinion_score > fact_score:
        return "è§‚ç‚¹", 0.7 + opinion_score * 0.1
    else:
        return "æ··åˆ", 0.6

# -------------------- å†…å®¹æå– --------------------
def extract_article(url: str):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0 Safari/537.36'
    }
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
    except requests.exceptions.RequestException as e:
        st.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{e}")
        return None

    soup = BeautifulSoup(resp.content, 'html.parser')
    article = type('MockArticle', (), {})()
    article.title = soup.title.text if soup.title else "æ— æ ‡é¢˜"

    body_text = []
    selectors = ['article', '.article-content', '.content', 'main',
                 'div.article-body', '.post-content', '.story-content']
    for sel in selectors:
        if els := soup.select(sel):
            body_text = [el.get_text(' ', strip=True) for el in els]
            break
    if not body_text:
        body_text = [p.get_text(' ', strip=True) for p in soup.find_all('p')]
    article.text = "\n\n".join(body_text)
    article.authors = []
    article.publish_date = None
    return article

# -------------------- åˆ†æä¸»æµç¨‹ --------------------
if st.button("å¼€å§‹åˆ†æ", type="primary") or url:
    if not url:
        st.warning("è¯·è¾“å…¥æ–°é—»ç½‘å€åå†ç‚¹å‡»åˆ†æ")
        st.stop()
    if not url.startswith(('http://', 'https://')):
        st.warning("ç½‘å€éœ€ä»¥ http:// æˆ– https:// å¼€å¤´")
        st.stop()

    with st.spinner('ğŸ” æ˜é‰´æ­£åœ¨åˆ†æä¸­...'):
        article = extract_article(url)
        if not article or len(article.text.strip()) < 50:
            st.error("æ— æ³•è§£æè¯¥ç½‘å€ï¼Œè¯·å°è¯•å…¶ä»–æ–°é—»ç½‘ç«™")
            st.info("ğŸ’¡ æç¤ºï¼šå»ºè®®ä½¿ç”¨ä¸»æµæ–°é—»ç½‘ç«™ï¼Œå¦‚æ–°æµªã€ç½‘æ˜“ã€äººæ°‘ç½‘ç­‰")
            st.stop()

        st.success(f"âœ… æˆåŠŸè·å–å†…å®¹ï¼š{article.title}")

        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**ä½œè€…ï¼š** {', '.join(article.authors) if article.authors else 'æœªçŸ¥'}")
        with col2:
            st.write(f"**å‘å¸ƒæ—¶é—´ï¼š** {article.publish_date or 'æœªçŸ¥'}")

        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\.]', article.text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        if not sentences:
            st.warning("æœªæå–åˆ°è¶³å¤Ÿå†…å®¹è¿›è¡Œåˆ†æï¼Œè¯·å°è¯•å…¶ä»–ç½‘å€")
            st.stop()

        total_sentences = min(50, len(sentences))
        results = []
        progress = st.progress(0)
        for i, sent in enumerate(sentences[:total_sentences]):
            label, conf = simple_analyze(sent)
            results.append({
                'åºå·': i + 1,
                'å†…å®¹': sent,
                'ç±»å‹': label,
                'ç½®ä¿¡åº¦': f"{conf:.2f}"
            })
            progress.progress((i + 1) / total_sentences)
        progress.empty()

        # ç»Ÿè®¡
        st.subheader("ğŸ“Š åˆ†ææ¦‚è§ˆ")
        fact_cnt = sum(1 for r in results if r['ç±»å‹'] == 'äº‹å®')
        opinion_cnt = sum(1 for r in results if r['ç±»å‹'] == 'è§‚ç‚¹')
        mixed_cnt = sum(1 for r in results if r['ç±»å‹'] == 'æ··åˆ')
        total_cnt = len(results)
        c1, c2, c3 = st.columns(3)
        c1.metric("äº‹å®é™ˆè¿°", f"{fact_cnt}æ¡", f"{fact_cnt / total_cnt * 100:.1f}%")
        c2.metric("è§‚ç‚¹è¡¨è¾¾", f"{opinion_cnt}æ¡", f"{opinion_cnt / total_cnt * 100:.1f}%")
        c3.metric("æ··åˆå†…å®¹", f"{mixed_cnt}æ¡", f"{mixed_cnt / total_cnt * 100:.1f}%")

        # è¯¦æƒ…
        st.subheader("ğŸ” è¯¦ç»†åˆ†æ")
        for r in results:
            label_emoji = {"äº‹å®": "âœ…", "è§‚ç‚¹": "ğŸ’¬", "æ··åˆ": "ğŸ”€"}
            card_class = {"äº‹å®": "lens-fact", "è§‚ç‚¹": "lens-opinion", "æ··åˆ": "lens-mixed"}
            st.markdown(f"""
            <div class="{card_class[r['ç±»å‹']]}">
                <strong>{label_emoji[r['ç±»å‹']]} {r['ç±»å‹']}</strong>
                <span style="float:right; color:#666">ç½®ä¿¡åº¦: {r['ç½®ä¿¡åº¦']}</span><br>
                {r['å†…å®¹']}
            </div>
            """, unsafe_allow_html=True)

        # ä¸‹è½½
        st.subheader("ğŸ“¥ å¯¼å‡ºç»“æœ")
        csv = pd.DataFrame(results).to_csv(index=False).encode('utf-8-sig')
        st.download_button("ä¸‹è½½åˆ†æç»“æœ(CSV)", csv, "æ˜é‰´åˆ†æç»“æœ.csv", "text/csv")

# -------------------- ä¾§è¾¹æ  --------------------
with st.sidebar:
    st.header("â„¹ï¸ å…³äºæ˜é‰´")
    st.markdown("""
    **æ˜é‰´ FactLens** æ˜¯AIé©±åŠ¨çš„æ–°é—»å†…å®¹åˆ†æå·¥å…·ï¼Œ  
    å¸®åŠ©ç”¨æˆ·å¿«é€ŸåŒºåˆ†æ–°é—»ä¸­çš„äº‹å®é™ˆè¿°ä¸è§‚ç‚¹è¡¨è¾¾ã€‚
    
    ### ä½¿ç”¨åœºæ™¯
    - ä¿¡æ¯çœŸå®æ€§éªŒè¯  
    - åª’ä½“ç´ å…»æ•™è‚²  
    - å†…å®¹åˆ†æç ”ç©¶  
    - ä¸ªäººé˜…è¯»è¾…åŠ©  
    """)

    st.divider()
    st.header("ğŸ¯ ä½¿ç”¨æŒ‡å—")
    st.markdown("""
    1. å¤åˆ¶æ–°é—»ç½‘å€  
    2. ç²˜è´´åˆ°è¾“å…¥æ¡†  
    3. ç‚¹å‡»ã€Œå¼€å§‹åˆ†æã€  
    4. æŸ¥çœ‹åˆ†æç»“æœ  
    5. å¯ä¸‹è½½è¯¦ç»†æ•°æ®  
    """)

    st.divider()
    st.header("ğŸŒ æ¨èç½‘ç«™")
    st.markdown("""
    ä»¥ä¸‹ç½‘ç«™å…¼å®¹æ€§è¾ƒå¥½ï¼š  
    - [æ–°æµªæ–°é—»](https://news.sina.com.cn)  
    - [ç½‘æ˜“æ–°é—»](https://news.163.com)  
    - [è…¾è®¯æ–°é—»](https://news.qq.com)  
    - [äººæ°‘ç½‘](http://people.com.cn)  
    - [æ–°åç½‘](http://www.xinhuanet.com)  
    """)

# -------------------- é¡µè„š --------------------
st.divider()
st.caption("Â© 2024 æ˜é‰´ FactLens - v1.0.0 - æ˜é‰´äº‹å®ï¼Œæ´è§è§‚ç‚¹")
