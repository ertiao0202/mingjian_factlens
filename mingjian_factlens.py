import streamlit as st
import requests
import re
import pandas as pd
from bs4 import BeautifulSoup
import time

# è®¾ç½®é¡µé¢ - ä½¿ç”¨å“ç‰Œæ ‡è¯†
st.set_page_config(
    page_title="æ˜é‰´ FactLens - äº‹å®è§‚ç‚¹åˆ†æ",
    page_icon="ğŸ”",
    layout="wide"
)

# å“ç‰Œå¤´éƒ¨çš„æ ·å¼
st.markdown("""
<style>
    .brand-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .fact-card { 
        background-color: #e6f7ff !important; 
        border-left: 4px solid #1890ff !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .opinion-card { 
        background-color: #fff7e6 !important; 
        border-left: 4px solid #ffa940 !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .mixed-card { 
        background-color: #f6ffed !important; 
        border-left: 4px solid #52c41a !important;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    @media (max-width: 768px) {
        .stColumn { width: 100% !important; }
        .brand-header h1 { font-size: 24px !important; }
        .brand-header h3 { font-size: 18px !important; }
    }
</style>
""", unsafe_allow_html=True)

# å“ç‰Œå¤´éƒ¨
st.markdown("""
<div class="brand-header">
    <h1>ğŸ” æ˜é‰´ FactLens</h1>
    <h3>æ˜é‰´äº‹å®ï¼Œæ´è§è§‚ç‚¹</h3>
    <p>AIé©±åŠ¨çš„æ–°é—»å†…å®¹æ™ºèƒ½åˆ†æå·¥å…·</p>
</div>
""", unsafe_allow_html=True)

# ä¸»è¦åŠŸèƒ½åŒºåŸŸ
st.header("ğŸ“° æ–°é—»åˆ†æ")

# è¾“å…¥ç½‘å€
url = st.text_input("è¯·è¾“å…¥æ–°é—»ç½‘å€ï¼š", placeholder="https://example.com/news/...")

# è§„åˆ™å¼•æ“å‡½æ•°
def simple_analyze(sentence):
    """ç®€å•è§„åˆ™åˆ†æäº‹å® vs è§‚ç‚¹"""
    sentence_lower = sentence.lower()
    
    # äº‹å®ç‰¹å¾
    fact_indicators = ['æ•°æ®', 'ç»Ÿè®¡', 'æŠ¥å‘Š', 'æ˜¾ç¤º', 'è¡¨ç¤º', 'è¯å®', 'æ®', 'ç ”ç©¶', 
                      'è°ƒæŸ¥', 'ç»“æœ', 'ç™¾åˆ†æ¯”', 'å¢é•¿', 'ä¸‹é™', 'æ—¥æœŸ', 'æ—¶é—´', 'åœ°ç‚¹']
    
    # è§‚ç‚¹ç‰¹å¾  
    opinion_indicators = ['è®¤ä¸º', 'è§‰å¾—', 'åº”è¯¥', 'å¯èƒ½', 'æˆ–è®¸', 'ç›¸ä¿¡', 'å»ºè®®', 'æˆ‘',
                         'æˆ‘ä»¬', 'å¿…é¡»', 'ä¸€å®š', 'è‚¯å®š', 'æ˜¾ç„¶', 'æ¯«æ— ç–‘é—®', 'ç¾å¥½', 'ç³Ÿç³•']
    
    fact_score = sum(1 for word in fact_indicators if word in sentence_lower)
    opinion_score = sum(1 for word in opinion_indicators if word in sentence_lower)
    
    if fact_score > opinion_score:
        return "äº‹å®", 0.7 + fact_score * 0.1
    elif opinion_score > fact_score:
        return "è§‚ç‚¹", 0.7 + opinion_score * 0.1
    else:
        return "æ··åˆ", 0.6

# ä½¿ç”¨BeautifulSoupçš„å†…å®¹æå–å‡½æ•°
def extract_article(url):
    """ä½¿ç”¨ BeautifulSoup æå–æ–°é—»å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„ article å¯¹è±¡
        class MockArticle:
            pass
        
        article = MockArticle()
        article.title = soup.title.text if soup.title else "æ— æ ‡é¢˜"
        
        # æå–æ­£æ–‡ - å¸¸è§æ–°é—»ç½‘ç«™é€‰æ‹©å™¨
        body_text = []
        selectors = [
            'article', 
            '.article-content', 
            '.content', 
            'div.content', 
            'main', 
            'div.article-body',
            '.post-content',
            '.story-content'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                body_text = [elem.get_text().strip() for elem in elements]
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–æ‰€æœ‰æ®µè½
        if not body_text:
            paragraphs = soup.find_all('p')
            if paragraphs:
                body_text = [p.get_text().strip() for p in paragraphs]
        
        article.text = "\n\n".join(body_text)
        article.authors = []
        article.publish_date = None
        return article
    except Exception as e:
        st.error(f"å†…å®¹æå–å¤±è´¥: {str(e)}")
        return None

if st.button("å¼€å§‹åˆ†æ", type="primary") or url:
    if url and url.startswith('http'):
        with st.spinner('ğŸ” æ˜é‰´æ­£åœ¨åˆ†æä¸­...'):
            try:
                # è·å–æ–°é—»å†…å®¹
                article = extract_article(url)
                
                if not article or not hasattr(article, 'text') or len(article.text.strip()) < 50:
                    st.error("æ— æ³•è§£æè¯¥ç½‘å€å†…å®¹ï¼Œè¯·å°è¯•å…¶ä»–æ–°é—»ç½‘ç«™")
                    st.info("ğŸ’¡ æç¤ºï¼šå»ºè®®ä½¿ç”¨ä¸»æµæ–°é—»ç½‘ç«™ï¼Œå¦‚æ–°æµªã€ç½‘æ˜“ã€äººæ°‘ç½‘ç­‰")
                    st.stop()
                
                st.success(f"âœ… æˆåŠŸè·å–å†…å®¹ï¼š{article.title}")
                
                # æ˜¾ç¤ºæ–‡ç« ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    if hasattr(article, 'authors') and article.authors:
                        st.write(f"**ä½œè€…ï¼š** {', '.join(article.authors)}")
                    else:
                        st.write("**ä½œè€…ï¼š** æœªçŸ¥")
                with col2:
                    if hasattr(article, 'publish_date') and article.publish_date:
                        st.write(f"**å‘å¸ƒæ—¶é—´ï¼š** {article.publish_date}")
                    else:
                        st.write("**å‘å¸ƒæ—¶é—´ï¼š** æœªçŸ¥")
                
                # åˆ†æå†…å®¹
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?]', article.text)
                sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
                
                if not sentences:
                    st.warning("æœªæå–åˆ°è¶³å¤Ÿå†…å®¹è¿›è¡Œåˆ†æï¼Œè¯·å°è¯•å…¶ä»–ç½‘å€")
                    st.stop()
                
                results = []
                total_sentences = min(50, len(sentences))
                
                # åˆ›å»ºè¿›åº¦æ¡
                progress_bar = st.progress(0)
                
                for i, sentence in enumerate(sentences[:total_sentences]):
                    label, confidence = simple_analyze(sentence)
                    results.append({
                        'åºå·': i+1,
                        'å†…å®¹': sentence,
                        'ç±»å‹': label,
                        'ç½®ä¿¡åº¦': f"{confidence:.2f}"
                    })
                    # æ›´æ–°è¿›åº¦
                    progress_bar.progress((i + 1) / total_sentences)
                
                # å®Œæˆè¿›åº¦æ¡
                progress_bar.empty()
                
                # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
                st.subheader("ğŸ“Š åˆ†ææ¦‚è§ˆ")
                fact_count = sum(1 for r in results if r['ç±»å‹'] == 'äº‹å®')
                opinion_count = sum(1 for r in results if r['ç±»å‹'] == 'è§‚ç‚¹')
                mixed_count = sum(1 for r in results if r['ç±»å‹'] == 'æ··åˆ')
                total_count = len(results)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("äº‹å®é™ˆè¿°", f"{fact_count}æ¡", f"{(fact_count/total_count*100):.1f}%")
                with col2:
                    st.metric("è§‚ç‚¹è¡¨è¾¾", f"{opinion_count}æ¡", f"{(opinion_count/total_count*100):.1f}%")
                with col3:
                    st.metric("æ··åˆå†…å®¹", f"{mixed_count}æ¡", f"{(mixed_count/total_count*100):.1f}%")
                
                # è¯¦ç»†åˆ†æç»“æœ
                st.subheader("ğŸ” è¯¦ç»†åˆ†æ")
                
                for result in results:
                    if result['ç±»å‹'] == 'äº‹å®':
                        st.markdown(f"""
                        <div class="fact-card">
                            <strong>âœ… äº‹å®</strong> <span style="float:right; color:#666">ç½®ä¿¡åº¦: {result['ç½®ä¿¡åº¦']}</span><br>
                            {result['å†…å®¹']}
                        </div>
                        """, unsafe_allow_html=True)
                    elif result['ç±»å‹'] == 'è§‚ç‚¹':
                        st.markdown(f"""
                        <div class="opinion-card">
                            <strong>ğŸ’¬ è§‚ç‚¹</strong> <span style="float:right; color:#666">ç½®ä¿¡åº¦: {result['ç½®ä¿¡åº¦']}</span><br>
                            {result['å†…å®¹']}
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div class="mixed-card">
                            <strong>ğŸ”€ æ··åˆ</strong> <span style="float:right; color:#666">ç½®ä¿¡åº¦: {result['ç½®ä¿¡åº¦']}</span><br>
                            {result['å†…å®¹']}
                        </div>
                        """, unsafe_allow_html=True)
                
                # ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ“¥ å¯¼å‡ºç»“æœ")
                df = pd.DataFrame(results)
                csv = df.to_csv(index=False).encode('utf-8-sig')
                
                st.download_button(
                    label="ä¸‹è½½åˆ†æç»“æœ(CSV)",
                    data=csv,
                    file_name="æ˜é‰´åˆ†æç»“æœ.csv",
                    mime="text/csv"
                )
                
            except Exception as e:
                st.error(f"åˆ†æå¤±è´¥ï¼š{str(e)}")
                st.info("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿ç½‘å€å¯å…¬å¼€è®¿é—®ï¼Œæˆ–å°è¯•å…¶ä»–æ–°é—»ç½‘ç«™")
    else:
        st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ç½‘å€ï¼ˆä»¥http://æˆ–https://å¼€å¤´ï¼‰")

# ä¾§è¾¹æ ä¿¡æ¯
with st.sidebar:
    st.header("â„¹ï¸ å…³äºæ˜é‰´")
    st.markdown("""
    **æ˜é‰´ FactLens** æ˜¯AIé©±åŠ¨çš„æ–°é—»å†…å®¹åˆ†æå·¥å…·ï¼Œ
    å¸®åŠ©ç”¨æˆ·å¿«é€ŸåŒºåˆ†æ–°é—»ä¸­çš„äº‹å®é™ˆè¿°ä¸è§‚ç‚¹è¡¨è¾¾ã€‚
    
    ### ä½¿ç”¨åœºæ™¯
    - ä¿¡æ¯çœŸå®æ€§éªŒè¯
    - åª’ä½“ç´ å…»æ•™è‚²
    - å†…å®¹åˆ†æç ”ç©¶
    - ä¸ªäººé˜…è¯»è¾…åŠ©
    """)
    
    st.divider()
    st.header("ğŸ¯ ä½¿ç”¨æŒ‡å—")
    st.markdown("""
    1. å¤åˆ¶æ–°é—»ç½‘å€
    2. ç²˜è´´åˆ°è¾“å…¥æ¡†
    3. ç‚¹å‡»ã€Œå¼€å§‹åˆ†æã€
    4. æŸ¥çœ‹åˆ†æç»“æœ
    5. å¯ä¸‹è½½è¯¦ç»†æ•°æ®
    """)
    
    st.divider()
    st.header("ğŸš€ æŠ€æœ¯ç‰¹æ€§")
    st.markdown("""
    - ğŸ” æ™ºèƒ½å†…å®¹æå–
    - ğŸ“Š å¤šç»´åº¦åˆ†æ
    - ğŸ“± å“åº”å¼è®¾è®¡
    - ğŸ”„ å®æ—¶å¤„ç†
    - ğŸ’¾ æ•°æ®å¯¼å‡º
    """)
    
    st.divider()
    st.header("ğŸŒ æ¨èç½‘ç«™")
    st.markdown("""
    ä»¥ä¸‹ç½‘ç«™å…¼å®¹æ€§è¾ƒå¥½ï¼š
    - æ–°æµªæ–°é—» (news.sina.com.cn)
    - ç½‘æ˜“æ–°é—» (news.163.com)
    - è…¾è®¯æ–°é—» (news.qq.com)
    - äººæ°‘ç½‘ (people.com.cn)
    - æ–°åç½‘ (xinhuanet.com)
    """)

# é¡µè„š
st.divider()
st.caption("Â© 2024 æ˜é‰´ FactLens - æ˜é‰´äº‹å®ï¼Œæ´è§è§‚ç‚¹")